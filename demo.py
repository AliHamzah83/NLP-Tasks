#!/usr/bin/env python3
"""
Arabic RAG System Demo Script

This script demonstrates the complete functionality of the Arabic RAG system
including data processing, knowledge base building, querying, and evaluation.
"""

import os
import sys
import time
import logging
from pathlib import Path

# Add the project root to the Python path
sys.path.append('.')

from arabic_rag import (
    ArabicRAGPipeline, 
    ArabicRAGEvaluator, 
    EvaluationMetrics
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def print_header(title: str):
    """Print a formatted header."""
    print("\n" + "="*60)
    print(f" {title}")
    print("="*60)

def print_step(step: str):
    """Print a formatted step."""
    print(f"\nğŸ”„ {step}")
    print("-" * 50)

def demo_basic_functionality():
    """Demonstrate basic RAG functionality."""
    print_header("Arabic RAG System Demo - Basic Functionality")
    
    try:
        # Step 1: Initialize the RAG pipeline
        print_step("Step 1: Initializing Arabic RAG Pipeline")
        rag = ArabicRAGPipeline(
            data_dir="data",
            db_path="demo_chroma_db",
            collection_name="arabic_demo",
            embedding_model_name="paraphrase-multilingual-mpnet-base-v2",
            llm_model_name="aubmindlab/aragpt2-base",
            chunking_strategy="sentence_based",
            chunk_size=512,
            chunk_overlap=50
        )
        print("âœ… RAG pipeline initialized successfully!")
        
        # Step 2: Build knowledge base
        print_step("Step 2: Building Knowledge Base")
        build_stats = rag.build_knowledge_base(force_rebuild=True)
        
        if build_stats.get('success', False):
            print(f"âœ… Knowledge base built successfully!")
            print(f"   ğŸ“Š Total documents: {build_stats.get('total_documents', 0)}")
            print(f"   ğŸ“„ Total chunks: {build_stats.get('total_chunks', 0)}")
            print(f"   â±ï¸  Build time: {build_stats.get('build_time_seconds', 0):.2f} seconds")
            print(f"   ğŸ§  Embedding model: {build_stats.get('embedding_model', 'N/A')}")
            print(f"   âœ‚ï¸  Chunking strategy: {build_stats.get('chunking_strategy', 'N/A')}")
        else:
            print(f"âŒ Knowledge base build failed: {build_stats.get('error', 'Unknown error')}")
            return
        
        # Step 3: Test queries
        print_step("Step 3: Testing Arabic Queries")
        
        test_questions = [
            "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
            "ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ",
            "Ù…Ø§ Ù‡ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©ØŸ",
            "ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©ØŸ"
        ]
        
        results = []
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ Query {i}: {question}")
            
            start_time = time.time()
            result = rag.query(
                question=question,
                top_k=5,
                max_contexts=3,
                similarity_threshold=0.3
            )
            query_time = time.time() - start_time
            
            if result.get('success', False):
                print(f"âœ… Answer: {result.get('answer', 'No answer')}")
                print(f"   ğŸ¯ Confidence: {result.get('confidence', 0):.2%}")
                print(f"   â±ï¸  Processing time: {result.get('processing_time', 0):.2f}s")
                print(f"   ğŸ“š Retrieved docs: {len(result.get('retrieved_docs', []))}")
                
                # Show top retrieved document
                retrieved_docs = result.get('retrieved_docs', [])
                if retrieved_docs:
                    top_doc = retrieved_docs[0]
                    print(f"   ğŸ” Top document similarity: {top_doc.get('similarity_score', 0):.3f}")
                    print(f"   ğŸ“„ Top document preview: {top_doc.get('text', '')[:100]}...")
                
                results.append(result)
            else:
                print(f"âŒ Query failed: {result.get('error', 'Unknown error')}")
        
        # Step 4: System statistics
        print_step("Step 4: System Statistics")
        stats = rag.get_system_stats()
        
        print("ğŸ“Š Vector Database:")
        vector_stats = stats.get('vector_database', {})
        print(f"   ğŸ“„ Total documents: {vector_stats.get('total_documents', 0)}")
        print(f"   ğŸ—‚ï¸  Collection: {vector_stats.get('collection_name', 'N/A')}")
        
        print("\nğŸ§  Embedding Model:")
        embedding_stats = stats.get('embedding_model', {})
        print(f"   ğŸ·ï¸  Model: {embedding_stats.get('model_name', 'N/A')}")
        print(f"   ğŸ“ Dimensions: {embedding_stats.get('embedding_dim', 0)}")
        print(f"   ğŸ’» Device: {embedding_stats.get('device', 'N/A')}")
        
        print("\nğŸ¤– Language Model:")
        llm_stats = stats.get('language_model', {})
        print(f"   ğŸ·ï¸  Model: {llm_stats.get('model_name', 'N/A')}")
        print(f"   ğŸ’» Device: {llm_stats.get('device', 'N/A')}")
        print(f"   ğŸ“ Max length: {llm_stats.get('max_length', 0)}")
        
        # Step 5: Performance summary
        print_step("Step 5: Performance Summary")
        if results:
            avg_confidence = sum(r.get('confidence', 0) for r in results) / len(results)
            avg_time = sum(r.get('processing_time', 0) for r in results) / len(results)
            success_rate = sum(1 for r in results if r.get('success', False)) / len(results)
            
            print(f"ğŸ“ˆ Average confidence: {avg_confidence:.2%}")
            print(f"â±ï¸  Average processing time: {avg_time:.2f}s")
            print(f"âœ… Success rate: {success_rate:.2%}")
        
        print_step("Demo Completed Successfully! ğŸ‰")
        
    except Exception as e:
        logger.error(f"Demo failed: {e}")
        print(f"âŒ Demo failed: {e}")

def demo_evaluation():
    """Demonstrate evaluation functionality."""
    print_header("Arabic RAG System Demo - Evaluation")
    
    try:
        # Initialize system
        print_step("Initializing System for Evaluation")
        rag = ArabicRAGPipeline(
            data_dir="data",
            db_path="demo_chroma_db",
            collection_name="arabic_demo"
        )
        
        # Build knowledge base if needed
        stats = rag.vector_db.get_collection_stats()
        if stats.get('total_documents', 0) == 0:
            print("Building knowledge base for evaluation...")
            rag.build_knowledge_base()
        
        # Initialize evaluator
        evaluator = ArabicRAGEvaluator(rag)
        
        # Create test dataset
        print_step("Creating Test Dataset")
        test_questions = [
            "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
            "ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ",
            "Ù…Ø§ Ù‡ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© ÙˆÙ…Ø§ Ø£Ù‡Ù…ÙŠØªÙ‡Ø§ØŸ",
            "ÙƒÙŠÙ ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"
        ]
        
        test_dataset = evaluator.create_test_dataset(
            questions=test_questions,
            generate_answers=True
        )
        print(f"âœ… Created test dataset with {len(test_dataset)} items")
        
        # Run evaluation
        print_step("Running Comprehensive Evaluation")
        metrics = evaluator.evaluate_end_to_end(
            test_dataset=test_dataset,
            save_results=True,
            output_path="demo_evaluation_results.json"
        )
        
        # Display results
        print_step("Evaluation Results")
        print("ğŸ¯ Retrieval Metrics:")
        print(f"   Precision: {metrics.precision:.4f}")
        print(f"   Recall: {metrics.recall:.4f}")
        print(f"   F1-Score: {metrics.f1_score:.4f}")
        
        print("\nğŸ“ Generation Metrics:")
        print(f"   Answer Accuracy: {metrics.answer_accuracy:.4f}")
        print(f"   Answer Completeness: {metrics.answer_completeness:.4f}")
        print(f"   Semantic Similarity: {metrics.semantic_similarity:.4f}")
        
        print("\nâš¡ Performance Metrics:")
        print(f"   Success Rate: {metrics.success_rate:.4f}")
        print(f"   Average Response Time: {metrics.average_response_time:.4f}s")
        print(f"   Confidence Score: {metrics.confidence_score:.4f}")
        
        # Generate report
        print_step("Generating Evaluation Report")
        report = evaluator.generate_evaluation_report(
            metrics=metrics,
            output_path="demo_evaluation_report.txt"
        )
        print("âœ… Evaluation report saved to 'demo_evaluation_report.txt'")
        
        print_step("Evaluation Demo Completed! ğŸ“Š")
        
    except Exception as e:
        logger.error(f"Evaluation demo failed: {e}")
        print(f"âŒ Evaluation demo failed: {e}")

def demo_document_management():
    """Demonstrate document management functionality."""
    print_header("Arabic RAG System Demo - Document Management")
    
    try:
        # Initialize system
        rag = ArabicRAGPipeline(
            data_dir="data",
            db_path="demo_chroma_db",
            collection_name="arabic_demo"
        )
        
        # Add custom documents
        print_step("Adding Custom Documents")
        
        custom_documents = [
            {
                "text": "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø¬Ø§Ù„ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ø§Ù… ØªØªØ·Ù„Ø¨ Ø°ÙƒØ§Ø¡Ù‹ Ø¨Ø´Ø±ÙŠØ§Ù‹. ÙŠØ´Ù…Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØ§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆØ§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª.",
                "metadata": {"title": "Ù…Ù‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "category": "ØªØ¹Ù„ÙŠÙ…ÙŠ", "author": "Ù†Ø¸Ø§Ù… RAG"}
            },
            {
                "text": "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ø°Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠÙØ³ØªØ®Ø¯Ù… ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ø«Ù„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙƒÙ„Ø§Ù… ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ©.",
                "metadata": {"title": "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚", "category": "ØªÙ‚Ù†ÙŠØ©", "author": "Ù†Ø¸Ø§Ù… RAG"}
            },
            {
                "text": "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ø£Ø¯ÙˆØ§Øª ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø®Ø§ØµØ© Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§. ØªØªÙ…ÙŠØ² Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒØ¨ÙŠØ±ØŒ ÙˆØ§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙ†ÙˆØ¹ ÙÙŠ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
                "metadata": {"title": "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©", "category": "Ø¨ÙŠØ§Ù†Ø§Øª", "author": "Ù†Ø¸Ø§Ù… RAG"}
            }
        ]
        
        success = rag.add_documents(custom_documents)
        if success:
            print(f"âœ… Added {len(custom_documents)} custom documents")
        else:
            print("âŒ Failed to add custom documents")
        
        # Test queries on new documents
        print_step("Testing Queries on New Documents")
        
        test_query = "Ù…Ø§ Ù‡ÙŠ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©ØŸ"
        result = rag.query(test_query, top_k=3)
        
        if result.get('success'):
            print(f"ğŸ“ Query: {test_query}")
            print(f"âœ… Answer: {result.get('answer', 'No answer')}")
            print(f"ğŸ¯ Confidence: {result.get('confidence', 0):.2%}")
            
            # Show retrieved documents
            retrieved_docs = result.get('retrieved_docs', [])
            print(f"ğŸ“š Retrieved {len(retrieved_docs)} documents:")
            for i, doc in enumerate(retrieved_docs):
                metadata = doc.get('metadata', {})
                title = metadata.get('title', 'Unknown')
                similarity = doc.get('similarity_score', 0)
                print(f"   {i+1}. {title} (similarity: {similarity:.3f})")
        
        # Export knowledge base
        print_step("Exporting Knowledge Base")
        export_success = rag.export_knowledge_base("demo_knowledge_base_export.json")
        if export_success:
            print("âœ… Knowledge base exported successfully")
        else:
            print("âŒ Failed to export knowledge base")
        
        # Get final statistics
        print_step("Final System Statistics")
        final_stats = rag.get_system_stats()
        vector_stats = final_stats.get('vector_database', {})
        print(f"ğŸ“Š Total documents in knowledge base: {vector_stats.get('total_documents', 0)}")
        
        print_step("Document Management Demo Completed! ğŸ“„")
        
    except Exception as e:
        logger.error(f"Document management demo failed: {e}")
        print(f"âŒ Document management demo failed: {e}")

def main():
    """Main demo function."""
    print_header("ğŸ” Arabic RAG System - Complete Demo")
    print("Welcome to the Arabic Retrieval-Augmented Generation System Demo!")
    print("This demo will showcase all major features of the system.")
    
    # Check if required directories exist
    os.makedirs("data", exist_ok=True)
    os.makedirs("exports", exist_ok=True)
    
    try:
        # Demo 1: Basic functionality
        demo_basic_functionality()
        
        # Demo 2: Document management
        demo_document_management()
        
        # Demo 3: Evaluation
        demo_evaluation()
        
        # Final summary
        print_header("ğŸ‰ Demo Summary")
        print("âœ… All demos completed successfully!")
        print("\nWhat was demonstrated:")
        print("1. ğŸ—ï¸  RAG pipeline initialization and configuration")
        print("2. ğŸ“š Knowledge base building from sample data")
        print("3. ğŸ” Arabic question answering with retrieval")
        print("4. ğŸ“„ Document management (add, export)")
        print("5. ğŸ“Š System evaluation and metrics")
        print("6. ğŸ“ˆ Performance analysis and reporting")
        
        print("\nGenerated files:")
        print("- demo_evaluation_results.json (evaluation metrics)")
        print("- demo_evaluation_report.txt (human-readable report)")
        print("- demo_knowledge_base_export.json (knowledge base export)")
        
        print("\nNext steps:")
        print("1. ğŸš€ Run the backend: python backend/main.py")
        print("2. ğŸŒ Run the frontend: streamlit run frontend/streamlit_app.py")
        print("3. ğŸ“– Check the README.md for detailed instructions")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Demo interrupted by user")
    except Exception as e:
        logger.error(f"Demo failed: {e}")
        print(f"âŒ Demo failed: {e}")

if __name__ == "__main__":
    main()